<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stochastic gradient descent | H. Kersting</title>
    <link>https://hanskersting.github.io/tag/stochastic-gradient-descent/</link>
      <atom:link href="https://hanskersting.github.io/tag/stochastic-gradient-descent/index.xml" rel="self" type="application/rss+xml" />
    <description>stochastic gradient descent</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 10 Jun 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hanskersting.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>stochastic gradient descent</title>
      <link>https://hanskersting.github.io/tag/stochastic-gradient-descent/</link>
    </image>
    
    <item>
      <title>New preprint on regularization properties of noise injection out!</title>
      <link>https://hanskersting.github.io/post/explicitnoiseinjection_paper_out/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://hanskersting.github.io/post/explicitnoiseinjection_paper_out/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.04613&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This new paper&lt;/a&gt; is a follow-up to our &lt;a href=&#34;https://arxiv.org/abs/2202.02831&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anti-PGD paper&lt;/a&gt; (ICML 2022). This time we show that small perturbations induce explicit regularization, which we spell out for a few models. Many new research avenues open up from our Theorem 2.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our paper on anticorrelated noise injection got accepted to ICML 2022</title>
      <link>https://hanskersting.github.io/post/antipgd_paper_accepted/</link>
      <pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate>
      <guid>https://hanskersting.github.io/post/antipgd_paper_accepted/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m happy that &lt;strong&gt;our paper on AntiPGD&lt;/strong&gt;, a version of stochastic gradient descent using anticorrelated perturbations, got accpeted to &lt;strong&gt;ICML 2022&lt;/strong&gt; in Baltimore, USA. You can read the PDF &lt;a href=&#34;https://arxiv.org/abs/2202.02831&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Looking forward to attend my first ML conference after Covid, present this paper, and have some scientific discussions.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
